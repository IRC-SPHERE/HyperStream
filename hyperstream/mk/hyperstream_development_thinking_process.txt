
are we working in local time or GMT time?

hyper-system
has its own (read-write-calc) hyper-streambases
can use hyper-streambases (read-only or read-calc-only) from other hyper-systems
has its own hyper-definition-stream (raw calc stream definitions) - a special stream
has its own hyper-tool-stream (python tool definitions, must not have side-effects)
has its own hyper-workflow-stream (python workflow definitions, must not have side-effects)
has its own hyper-script-stream (python scripts with commands, the system executes these as they appear)
script-stream could be called control-stream
workflow-stream is purely for better code management, these commands could be directly launched from script-stream
maybe workflow-stream should be merged with script-stream - they are just defs in the script-scream?

requirement of repeatability:
running all the scripts of the hyper-script-stream must always give the same result, regardless of time of run:
- the same set of streams created (identifiers can differ, but names must be the same)
- in any time-range where these streams are calculated, the documents must be identical (if stochastic, then these must be identical generative models)
note that if a script is run more than once in the script stream then the effects can vary between these two runs
mathematically, results must be independent of clock time

stronger requirement of replicability
- requires that all tools were non-stochastic

3 different timestamps/intervals:
- STREAM-TIME: current time interval in the stream for which a document is being calculated (defines which parts of input streams are used)
- SCRIPT-TIME: the time when the current running script was entered into the hyper-script-stream (defines which versions of code and other similar streams are being used)
- CLOCK-TIME: the actual current clock (used to insert documents into the system log files)

types of streams:
- EXTERNAL STREAM (all hyper-systems have read-only access)
- DERIVED STREAM (one system has read-write access)
- SYSTEM STREAM (user has read-only access, system has read-write access) PROBABLY WILL NOT BE USED, instead log files, which are not streams per se
all streams in each streambase must have the same type
i.e., stream type is determined by the streambase that it belongs to

script commands:
run a workflow
calculate a stream in some time interval
run garbage collection
invalidate some stream after some time-point (require re-calculation of all dependencies)
- think more about that
- if some input stream gets deleted then all derived streams must become stored to ensure repeatability
- if this stream has been used in another hyper-system, then that system needs to get invalidate-commands in its script-stream as well
- perhaps introduce an invalidation stream into each hyper-system, then this can be checked every now and then

how to deal with equivalences of two versions of a tool

during defining a stream all parameters of fixed time ranges can be already replaced with actual data, e.g. the tool reference
where are the parameters stored? json! (or bson)
when are stream definitions identical? 
- if the respective json is identical
- if the jsons are equivalent (a checker)
equivalences must be documented together with the tool
- where and how?
- must be hashed without the things that don't matter, so that can be found later
- equivalences are a matter of efficiency, no effect on the content

streambase optimisation procedure: 
replace static streamrefs within streamdefs by actual data, if possible
then rehash them within the streambase

how to use streams defined by earlier scripts?
- by name (but names can be overwritten, might introduce bugs - do we require script id as well?)
- don't allow overwriting of names?
- introduce global and local names?
any workflow is run in its definition time, i.e. any global names used are resolved according to that time
if up-to-date meanings required, then workflow must have parameters (or code copied rather than called)
that is, any code is run in with stream names resolved in definition-time and stream data resolved in call-time
if a workflow calls some tool and that tool gets updated, then the workflow starts calling the new tool
but if a stream name gets overwritten, then the workflow still refers to the old name
thus, all stream names are global but can be used locally without worrying about these getting overwritten later
i.e. a stream directly referred to from a workflow (e.g. F['streamname']) will always remain the same stream

tool default params must be resolved before checking whether the stream is already defined

localise workflow used in the phantom removal workflow - how to update the localisation part?
workflow used as a tool
but it is hard to trace the dependencies of the workflow - it can depend on many streams, linked through both parameters and global names
still, tool updates and workflow updates should be dealt with in a similar or same manner?
when a workflow uses another workflow, then the name of the workflow in use will always refer to the same workflow stream (definition-time), but the actual workflow called in that stream will be decided call-time
or maybe the workflow itself can choose the way it calls the other workflow. the default choice is call-time, but alternatives are definition-time and delta(0)

tool versions 1.0 and 1.0.0 and 1.0.1 could give same output, 1.1 can give different output

TODO: how is data_extractor different from get_stream_reader:
perhaps should unify these two?
what is the calculation logic? how is calculation split into pieces? by whom?
relevant information owned by tool, engine, stream_def, stream_ref
need to calc a long stretch
full information stored in stream_def

definition-time (early) calculation of parameters vs calculation-time (late)
must be under the control of the coder
e.g. M['x'] is late, M['x']() is early
with a tool stream, is there a difference between T['x']() and T['x']()() ?
how to decide whether to return [doc,doc,...] or [data,data,...]
how to decide whether to return [data] or data
perhaps this should already be defined by the stream_ref itself?
and early vs late should be defined by how used
but still, how to split calculations?
even if the streamref specifies usage as unpacked data, the tool gets run on a timerange
actually, how can a tool be sure that there is exactly one document coming from one input stream_ref? e.g. if timed stream.
unpacking is a convenience which we may choose not to do
for instance, require all parameters to be lists, then each tool needs to do unpacking on all parameters
but we don't want to write usual parameters in a list enclosure
remaining to decide: early vs late, [docs] vs [data]
e.g. M['x'] is late, M['x']() is early
only need to deal with the exception in the case of a tool applied on other things
maybe it's ok to use the tool as M['x']()(params)
but it only works if () takes 'data' out instead of all other options
regarding [docs] vs [data] we could be very explicit: M['x',doc] and M['x',data]
data of the last document - could be special? alternatively, data of the first doc
actually, perhaps tool is always late calculation? because we store the reference not the actual code!
we want to maximally reuse results, therefore it is important to declare, that the tool only depends on the data and not on the timestamps
for early calculation we just need efficient way of writing down inline fetching of data from stream_refs:
e.g. .last_doc() .doc_list() .value_list() .last_value() .first_doc() .first_value()
and why not .doc_gen() and .doc_tuple() and .doc_dict()
there is still issue regarding what late calculation results in
we want to make sure that we treat those same options as different inputs
essentially, they are filters then and should be part of the stream_ref
looks like late vs early and [docs] vs [data] vs doc vs data is solved:
we introduce () to enforce early calculation and filters for the latter problem
tool outputs a stream_def
that calculation can have modifiers / parameters (...)
workflow outputs a list of stream_defs, the user can treat the last one as the result of the workflow (TODO: rethink this later)
stream_def becomes a stream after getting a streambase and stream_id
is all the above ok with relative and absolute stream_refs?
relative stream_refs cannot be calculated, they must be made absolute before
a stream_def can include both relative and absolute stream_refs
how to calculate a stream with a particular stream_def and absolute time range?
- take the whole range together, make relative stream_refs into absolute
- as a result, all required data are there
- but the tool requires not only the data, but the original relative stream_refs as well!
- because the stream_def is essentially point-wise
- so required are the data and the stream_def
- basically, some parameters of the tool are buried inside stream_defs
- one possibility: provide both stream_def and data within it, the tool does it all
- note that the filters/modifiers must be applied only within the tool, because they must be applied within the relative stream_ref window within the long stretch of data that needs calculation
- but there can be another type of filters which don't depend on the window, they can be applied earlier
- define filter chains with the + operator?
- to be precise, within a tool one has to run a loop through all microseconds of the time range, for each microsecond make the relative stream_refs into absolute, get data through the filters, apply the tool to create documents into this microsecond 
- need to find useful subcategories of tools which allow easier calculation processes and build helpful wrappers for each of those subcategories
- TODO: revisit this later once the subcategories have emerged and see if we can optimise the calculation processes in an even higher level, by choosing the batch size
B['stream',start,end,modifiers](modifiers2) means: read the data out from the stream from the (start,end] interval and apply modifiers to obtain the stream_ref B['stream',start,end,modifiers]; this stream_ref is from a particular streambase and the streambase determines what the meaning of modifiers2 is and how they are applied
in the toolbase modifiers2 act as input data to the tool determined by the stream_ref and together result in a stream_def

StreamRef.__call__:
TODO: ask streambase to give data for this stream_ref with these args and kwargs
streambase knows how to fulfill this request:
if data are immediately available (calculated or read-only), then provide the data
if not, then forward the request to its dedicated engine
the engine 
the args and kwargs might include callbacks
if callbacks are not there then wait until data arrives
the engine must support both immediate calculation and putting in the queue
the engine is fulfilling requests to many streambases
is there value in having a single engine vs one per streambase?
there are dependencies between streams in different streambases, can easily happen that one needs to wait after the others
perhaps better to have one central queueing system rather than many smaller ones
parameters in such a request:
- success callback with the result generator as a parameter
- failure callback
- timeout - if not done by that time then stop


